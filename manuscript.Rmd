---
layout: review, 11pt
linenumbers: true
title: "R tools for accessing research literature for text mining"
author:
  - name: Scott Chamberlain
    affiliation: cstar
    email: myrmecocystus(at)gmail.com
    footnote: Corresponding author
address:
  - code: cstar
    address: |
      rOpenSci, Museum of Paleontology, University of California, Berkeley, CA, USA
abstract: |
  Text mining is a powerful method for answering research questions. However,
  getting texts to extract information can be a daunting and complicated task.
  The primary reason for this is the diversity of publisher technologies.
  There are thousands of different publishers, each with their own licenses,
  URL patterns, access options, and more. Layered on top of that is the varied
  access each user has based on their institutional affiliation. Here, I
  introduce a suite of software packages in the R programming language for
  fetching texts. The tapestry of different publishers, access levels, and
  other factors requires a patchwork of approaches for getting texts to users.
  The flagship R package called fulltext attempts to simplify search and
  retrieval of texts for text mining by serving as an interface to the varied
  and complex publishers. The fulltext package, along with many others, make
  acquiring texts easier than ever, facilitating answering research questions
  with text mining.
bibliography: components/references.bib
csl: components/peerj.csl
documentclass: components/elsarticle
output:
  pdf_document:
    template: components/elsarticle.latex
    keep_tex: true
    fig_caption: true
---


```{r compile-settings, include=FALSE}
library("methods")
library("knitr")
opts_chunk$set(
  tidy = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = 1,
  comment = '#>',
  collapse = TRUE,
  verbose = TRUE
)

basename <- gsub(".Rmd", "", knitr:::knit_concord$get('infile'))
opts_chunk$set(fig.path = paste("components/figure/", basename, "-", sep=""),
               cache.path = paste("components/cache/", basename, "/", sep=""))

# tibble options
options(tibble.max_extra_cols = 10)
```

\newpage

# Introduction

There's more than 100 million research articles published
(Crossref API: <https://github.com/CrossRef/rest-api-doc>), representing
an enormous amount of knowledge. In addition to simply reading these
articles, the articles contain a vast trove of information of interest to
researchers for machine aided questions [@Kong2018; @Usai2018]. For example,
many researchers are interested in statistical outcomes of articles that
can be extracted from numeric results: P-values, effect sizes, means, and
more. In addition, researchers are often interested in words in articles,
their use through time, and the contexts they are found in.

Text mining is the broad term associated with pulling information
out of articles. Given the importance of text mining, good
text mining tools are needed to make it easier for researchers to do.
Graphical user interface (GUI) based text mining tools are available
[e.g., @ba2016interoperability; @Munoz2019; @Canada2017] and some research papers
have used them [@Chaix2019], but given the urgent recent call to action for
more reproducible research [@OSC2015; @Camerer2016; @Camerer2018],
we must move away from GUI based tools as fast as possible. A number of examples
of programmatic tools can be found in the literature. For example, @Sinclair2016
present a tool in Python called seqenv for the domain specific task of
linking sequences to environments through text mining.

Most recent text mining papers do not use programmatic approaches, highlighting
the need for more programmatic text mining tools, and increased
discussion of those tools to increase awareness. For example, many papers
search Web of Science using their web interface, and downloading papers manually
[@Ding2018; @McCallen2019]. Many of these papers doing GUI based searching and
paper downloads are using R or Python downstream for analysis; replacing
GUI based data acquisition with programmatic approaches will improve
research.

The R programming language is free of cost, and is used widely throughout
many academic fields; tools in R for text mining are of particular
importance because they can be adopted by academics rapidly.

Here, I present an overview of text mining tools in the R programming
language, not for text mining analysis, but rather those
tools for searching for, acquiring, and extracting parts of texts
(e.g., title, abstract, authors). Most of the packages presented here are
part of the rOpenSci suite (https://ropensci.org/).


# Digital articles: technical aspects

Those articles that are digital (which in theory includes all articles)
can be split into two groups: machine readable and non-machine
readable.

The machine readable articles are those in XML[^xml], JSON[^json], or
plain text format. The former two, XML and JSON, are the best machine
readable types because they are structured data[^sd], whereas
plain text has no structure - it's simply a set of characters
with line breaks and spaces in between.

Of the non-machine readable types, the most noteable is the Portable
Document Format (PDF)[^pdf]. These can be broken out into two groups: text based
PDFs and scanned PDFs (images of text). The former are converted from digital
versions of various kinds (MS Word, OpenOffice, LaTeX, markdown, etc.), while the
latter are created by scanning print articles to a PDF format. Text-based
PDFs are much better for text mining purposes as plain text can be extracted
easily in R with [pdftools][], a binding to [libpoppler][]. However, with
scanned PDFs, text must be extracted using Optimal Character Recognition
(OCR; see R package [tesseract][]), which isn't always a clean solution,
especially compared to true text based PDFs.

The reality in scholarly publishing is all publishers, if they provide
any access to their articles, only provide PDF format. Very few publishers,
with some quite large (Elsevier, Pensoft, PLOS), provide XML format. Although
most publishers most likely have the XML behind each of their articles,
they for some indefensible reason do not share it - making text mining more
difficult. Some provide plain text (Elsevier). I only know of one publisher
that provides full text as JSON (PLOS). Thus, text mining, in most cases,
will require extracting text from PDFs.

[^xml]: https://www.w3.org/TR/xml/
[^json]: https://tools.ietf.org/html/rfc7159
[^sd]: https://en.wikipedia.org/wiki/Data_model
[^pdf]: https://en.wikipedia.org/wiki/PDF

# Digital articles: the access landscape

Acces to full-text is the holy grail in text mining. Some use cases can
get by with article metadata (authors, title, etc.), some with abstracts,
but many use cases require full-text.

The landscape of access to full-text is extremely hetergeous, with the
majority of variation along the publisher axis. The major hurdle is
paywalls. The majority of articles are published by the big three
publishers - Wiley, Springer, Elsevier - and the majority of their articles
are behind paywalls.

A promising sign is an increasing number of open access articles, yet
open access articles represent a small percent of all articles: an estimate in
2018 said that 28% of the scholarly literature was open access [@Piwowar2018].

With respect to paywalled articles, access varies by institution, depending
on each institution's publisher contracts. MORE ABOUT THIS ...

Some may not realize access to articles varies with IP address so that
access from campus vs. from home (if not on a VPN) will drastically differ.
Sometimes a VPN is required, and this can provide a significant technical
hurdle to users attempting to do text mining work.

One final hurdle in text mining comes unsurprisingly from Elsevier. They
use so-called "fences" for programmatic access. That is, even if a person
trying to get an article programmatically their institution has access to
and they have access to, and they are on the correct IP address, they may
still not get access to an Elsevier article. Elsevier puts in place these
fences and only if you contact their technical team directly can you
get these fences removed, and only then on a per institution basis.

I can not end this section without mentioning SciHub. This is a last
resort option for many probably (or possibly first, depending on your level
of access), providing access to full text of articles that are normally
paywalled. No tools in this manuscript provide access to SciHub.

# The discovery problem

A text mining project starts with a question. From that question, researchers
then attempt to acquire scholarly articles for text mining. Finding appropriate
articles is not altogether straight-forward.

Some of the discovery difficulty relates to the fact that there are so many
places to search for articles; a non-exhaustive list:
Google Scholar, Microsoft Academic Research, Scopus, ScienceDirect,
Web of Science, Pubmed/Entrez, Europe PMC, Directory of Open Access Journals,
Open Knowledge Maps, CORE, Fatcat, and more. It's probably difficult to know
where the best place is to search. Some of these are paywalled
(e.g., Web of Science), and some are not.

The most important aspect about any source for article search with respect to
reproducible research is being able to use the data source programmatically.
Of those listed above, the following can  be used programmatically:
Microsoft Academic Research, Scopus, ScienceDirect, Pubmed/Entrez, Europe PMC,
and Directory of Open Access Journals. All of these are included in the R
package [fulltext][], discussed further below.

On top of the vast array of different data sources is the varied ways that
search is implemented in each source. Most sources are probably using Solr
or Elasticsearch under the hood, though we can't know this for sure as most
do not make their software infrastructure public knowledge. Nonetheless, data
sources differ in how search works from the user perspective. For example, some
provide wild card/fuzzy search (i.e., 'appl\*' includes results for 'apple',
'application', etc.) and some do not. Some sources are searching full text
of articles, while others only search metadata (i.e., title, authors, abstract).
In addition, each source has a different set of metadata/full text available.
In brief, the same search against different sources produces different results.
Some text mining research articles perform the same search against many
different sources (refs), while others choose just one source.


# Data sources

There is increasing open access scientific literature content available
online. However, only a small proportion of scientific journals provide
access to their full text; whereas, most publishers provide open
access to their metadata only (most often through Crossref; Table 1).
The following is a synopsis of the major data sources and associated R
tools.

\newpage
Table 1. Sources of scientific literature, their content type provided via
web services, whether rOpenSci has an R packages for the service, and
where to find the API documentation.

Data Provider                      | Content Type                 | rOpenSci Package   | Documentation
---------------------------------- | ---------------------------- | ------------------ | -------------
Crossref                           | Metadata                     | rcrossref/crminer  | [^1]
DataCite                           | Metadata                     | rdatacite          | [^2]
Biodiversity Heritage Library      | Full text/Metadata           | rbhl               | [^3]
Public Library of Science (PLoS)   | Full text (pdf/xml)/Metadata | rplos              | [^4]
Scopus (Elsevier)                  | Full text (pdf/xml)/Metadata | fulltext           | [^5]
arXiv                              | Full text (pdf)/Metadata     | aRxiv              | [^6]
Biomed Central (via Springer)      | Full text (pdf)/Metadata     | fulltext           | [^7]
bioRxiv                            | Full text (pdf)/Metadata     | fulltext           | [^8]
PMC/Pubmed (via Entrez)            | Full text (pdf/xml)/Metadata | rentrez            | [^9]
Europe PMC                         | Full text (pdf/xml)/Metadata | europepmc          | [^10]
Microsoft Academic Search          | Metadata                     | microdemic         | [^11]
Directory of Open Access Journals  | Metadata                     | jaod               | [^12]
JSTOR Data for Research            | Full text                    | jstor              | [^13]
ORCID                              | Metadata                     | rorcid             | [^14]
Wikimedia's Citoid                 | Citations                    | rcitoid            | [^15]
Open Citation Corpus               | Citations                    | citecorp           | [^16]
Fatcat                             | Metadata                     | none               | [^17]
SHERPA/RoMEO                       | Journal Level Metadata       | rromeo             | [^18]
CORE                               | Full text (pdf)/Metadata     | rcoreoa            | [^19]
Dissemin                           | Metadata                     | dissemr            | [^20]

[^1]: https://api.crossref.org
[^2]: https://support.datacite.org/docs/api
[^3]: http://bit.ly/KYQ1Rd
[^4]: http://api.plos.org/solr
[^5]: http://bit.ly/J9S616
[^6]: https://arxiv.org/help/api/index
[^7]: https://dev.springer.com/
[^8]: http://www.biorxiv.org/
[^9]: https://www.ncbi.nlm.nih.gov/books/NBK25500
[^10]: https://azure.microsoft.com/en-us/services/cognitive-services
[^11]: https://dev.labs.cognitive.microsoft.com/docs/services/56332331778daf02acc0a50b/operations/565d9001ca73072048922d97
[^12]: https://doaj.org/api/v1/docs
[^13]: https://www.jstor.org/dfr/
[^14]: https://pub.orcid.org/
[^15]: https://en.wikipedia.org/api/rest_v1/#/Citation/getCitation
[^16]: http://opencitations.net/
[^17]: https://fatcat.wiki/
[^18]: http://www.sherpa.ac.uk/romeo/apimanual.php?la=en&fIDnum=|&mode=simple
[^19]: https://core.ac.uk/
[^20]: https://dissemin.readthedocs.io/en/latest/api.html

## Crossref and Datacite

Crossref is a non-profit that creates (or "mints") Digital Object Identifiers (DOIs).
In addition, they maintain metadata associated with each DOI. The metadata ranges
from simple (including author, title, dates, DOI, type, publisher) to including
number of citations to the article, as well as references in the article, and even
abstracts. At the time of writing they hold 100 million DOIs.

One can search by DOI or search citation data to get citations. In addition, Crossref has a
text mining opt-in program for publishers. The result of this is that some
publishers provide URLs for full text content of their articles. The majority of these links
are pay-walled, while some are open access. Using any of the various tools for working with
Crossref data, you can filter your search to get only articles with full text links, and further
to get only articles with full text links that are open access.

The main interfaces for Crossref in R are [rcrossref][] and [crminer][]. rcrossref is a
complete client for the public facing Crossref web services including metadata, whereas crminer
focuses only on retrieving full text of articles. Similar interfaces to rcrossref
are available in Ruby ([serrano][]) and Python ([habanero][]).

Datacite is similar to Crossref, but focuses on datasets instead of articles. The main
interface for Datacite in R is [rdatacite][].

## Biodiversity Heritage Library

The Biodiversity Heritage Library (BHL) houses scans of biodiversity books, and provides
web interfaces and APIs to query and fetch those data. They also provide text of the scanned
pages. The main R interace to BHL is through [rbhl][].

## Public Library of Science

The Public Library of Science (PLOS) is one of the largest open access only publishers. They
as of this writing have published 2.1 million articles. One of the strongs advantages of PLOS
is that they provide an API to their Solr instance, which is a very flexible way to
search their articles. The main R interace to PLOS is through [rplos][].

## Elsevier/Scopus

Elsevier is one of the largest publishers. Scopus is one of their products, a searchable
database of articles. Most of their articles are not open access. However,
they have a number of advantages if you have access to their articles: they are one of the few
publishers to provide machine readable XML (many publishers do have XML versions of articles,
but do not provide it, as mentioned in the Introduction); they are one of the few (two)
publishers that take part in Crossref's text and data mining program. The packages
[fulltext][] and [crminer][] can be used to access Elsevier articles through Crossref's
TDM program. There's an interface to Scopus article search within [fulltext][].

## arXiv/bioRxiv

arXiv and bioRxiv are preprint publishers, the former in existence for many years, and the
latter new on the scene. Preprints are scholarly articles that are generally not
peer-reviewed, but that for the most part will later be published in a different peer-review
publication. You can access articles from these publishers through [fulltext][].
arXiv does provide a web API that we hook into; bioRxiv does not, but we can get you
articles nonetheless.

## Pubmed/PMC/Europe PMC

Pubmed/PMC is a corpus/website of NIH funded research in the United States; while Europe PMC
is an equivalent for the European Union. You can access articles from Pubmed/PMC through
[fulltext][], and for Europe PMC through [europepmc][].

## Microsoft Academic Research

Microsoft Academic Research (MAR) is a search engine for research articles. You can use their
GUI web interface to search, and they provide APIs for programmatic access. The R interface
for MAR is [microdemic][]; and [fulltext][] hooks into `microdemic` as well for article search
and abstract retrieval.

## Directory of Open Access Journals

Directory of Open Access Journals (DOAJ) maintains data on open access journals, as well
as some portion of the articles in those journals. Thus, you can search for journals as
well as articles with DOAJ. The R interface for DOAJ is [jaod][].

## JSTOR

JSTOR's Data for Research program gives institutions with access to JSTOR, access
to full text of articles within JSTOR. There is no way however to make the interaction
with JSTOR completely programmatic, thus making reproducible research very difficult.
Nonetheless, there is an R package ([jstor][]) for using data from JSTOR's Data
for Research.

## ORCID

ORCID (https://orcid.org/) is an organization keeping track of identifiers and metadata
for researchers around the world. Individuals can optionally maintain metadata on their
scholarly works connected to their account with ORCID. Thus, across all of ORCID, a
significant cache of metadata is accruing on scholarly works, their funding amounts,
collaborators, etc., useful for bibliometrics research and more. The R interface for
ORCID is [rorcid][].

## Citoid/Open Citation Corpus

The Open Citation Corpus (http://opencitations.net/) holds records of which articles
cite which other articles, allowing for all important research on the scholarly
web of citation. Citation data has been very closely guarded until recently, but the
largest publishers are still not contributing to the Open Citation Corpus. The R
interface to the Open Citation Corpus is [rcitoid][].

## Fatcat

Fatcat is a project from Ben Newbold of the Internet Archive Labs. It is a
"versioned, publicly-editable catalog of research publications:  journal articles,
conference proceedings, preprints, blog posts". Fatcat is currently does not have an
R client, but is used inside of the [fulltext][] package.

## SHERPA/RoMEO

SHERPA/RoMEO (http://sherpa.mimas.ac.uk/romeo/index.php) aggregates and analyses
publisher open access policies and provides summaries of self-archiving permissions
and conditions of rights given to authors. The [rromeo][] is an R interface
to SHERPA/RoMEO.

## CORE

CORE (https://core.ac.uk/) touts itself as the world's largest collection of open
access research articles, providing metadata on journals and articles, as well
as access to the full text of articles. The [rcoreoa][] R package interfaces
with the CORE API.

## Dissemin

Dissemin (https://dissem.in/) detects papers behind pay-walls and invites authors
to upload them to an open repository. Dissemin provides metadata including links to
open versions of articles. The [dissemr][] R package interfaces with the
Dissemin API.



# fulltext: a toolset for text mining in R

[fulltext][] is a general purpose R package for the data part of text mining:
search for articles, get links to articles, get article abstracts, and fetch
full text of articles. The `fulltext` package is always adding additional
data sources as time allows (See Table 1). Starting from searching for
articles, the outputs of search can be fed into a function to get links
to those articles, or to get abstracts for those articles, or to fetch
their full text. The following is a breakdown of the major distinct
parts of `fulltext`.

## Search

`ft_search()` provides search access to nine different data sources (PLOS, BMC, Crossref,
Entrez, arXiv, bioRxiv, Europe PMC, Scopus, Microsoft Academic), creating a
mostly unified interface to all data sources. The parts of each data source
that are common are for the most part factored out into the parameters of the
`ft_search()` function: query term(s), pagination (number of results, result
number to start at). In addition, we allow the user to pass on data source specific
options to refine the search per data source.

With `ft_search()`, you can query any combination of the nine data sources at once.
The returned object is a list, with access to results of each data source by its name
(e.g., `$plos`, or `$crossref`). For each data source, the returned object does vary
because the returned data from each data source widely varies; for the most part
data.frame's are returned. For those data sources not queried, their slot is empty.

One important aspect of the research result we highlight is the licenses in the
returned data for each data source.

```{r}
x <- ft_search(query = 'ecology', from = c("plos", "crossref"))
```

The results for this PLOS search have all CC-BY licenses


```{r}
x$plos
```

Whereas the results for this Crossref search have mixed licenses

```{r}
x$crossref
```

You can dig into the license field for each article, with URLs holding information
on each license

```{r}
vapply(x$crossref$data$license, function(w) w$URL[1], "")
```

## Links

`ft_links()` provides two pathways to get links (URLs) for articles, with a choice
of four different data sources (PLOS, BMC, Crossref, Entrez). First, you can
use `ft_search()`, then pass the output of that function to `ft_links()`.

```{r}
out <- ft_search(query = "ecology", from = "entrez")
ft_links(out)
```

Second, you can pass DOIs directly to `ft_links()`. Both end up at the same point, links
for each article, if they could be found for the user selected data source.

```{r eval=FALSE}
# FIXME
ft_links(out$entrez$data$doi)
```

The biggest caveat with `ft_links()` is that we can't gaurantee that the links
will work. Link rot is one way in which the links may not work: link rot is when
the URL does not point to the original content anymore, or fails altogether.
Additionally, with Crossref, publishers can deposit URLs for articles, but they
make change the URLs at some later date but not update the URLs with Crossref.

## Abstracts

`ft_abstract()` provides access to article abstracts from four different
data sources (PLOS, Scopus, Microsoft Academic Research, Crossref). The only
way to use the function is to pass article identifiers, which are for the most
DOIs.

The advantage of abstracts over full text is that abstracts can often be
retrieved even for paywalled articles. That is, you can have much broader
coverage of the articles you're targeting relative to full text.

If you are after abstracts, and you are already getting or already have full text,
and if the articles are in XML format, then you can use [pubchunks][] to extract
out the abstracts.

## Fetch full text

`ft_get()` fetchs full text of articles from many different data sources. From the DOIs
that are passed in to the function, we detect the publisher, and there are specific plugins
for certain publishers: AAAS, American Institute of Physics, American Society of
Clinical Oncology, American Society for Microbiology, arXiv, bioRxiv, BiomedCentral,
Copernicus, Crossref, Elife, Elsevier, Pubmed/PMC via NCBI's Entrez, Frontiers, IEEE,
Informa, Instituto de Investigaciones Filologicas, American Medical Association,
Microbiology Society, PeerJ, Pensoft, PLOS, PNAS, Royal Society of Chemistry,
ScienceDirect, Scientific Societies, and Wiley.

If there's no built-in plugin for the publisher already, we use the FTDOI API
(<https://ftdoi.org>) to try to get the link for the full text of the article. If
the FTDOI API doesn't bear fruit, we search Crossref for a link to the full text.
If Crossref doesn't have any full text links, we give up.

Since users can go through a lot of article requests, we cache successfully
downloaded articles, and keep that knowledge consistent across R sessions; all
subsequent requests for the same article just use the cached version. Additionally,
all errors in `ft_get()` are collected in a tidy data.frame in the output of the
function to help the user quickly determine what went wrong.


# How to text mine from R: Three case studies

## Case study 1: Citation mining

```{r citation-mining, child="use-cases/citation-mining.Rmd", eval=TRUE}
```

## Case study 2: Abstract mining

```{r abstract-mining, child="use-cases/abstract-mining.Rmd", eval=TRUE}
```

## Case study 3: Full text mining

```{r full-text-mining, child="use-cases/full-text-mining.Rmd", eval=TRUE}
```


# Future directions

Text mining will always be a complex task given all the layers involved:
often temporal time-span of research questions; varied permissions among
researchers and their articles they're trying to access; varied approaches
to getting full text (xml vs pdf vs plain text); and more.

Programmatic text mining is a first step towards making text mining easier.
The R ecosystem is an especially good place to do text mining because
there are many packages for text mining analysis, and endless packages for
any required statistical analyses. In addition, rOpenSci and others are
building up a set of packages in R for searching for and acquiring full
text programatically to help make the research workflow as reproducible
as possible.

Future work for `fulltext` includes:

1. Adding more publisher plugins
2. Fine tuned user control over publishers
3. Improve VPN/proxy controls
4. Incorporate more search engines to help resolve URLs for fulltext versions
5. Improve documentation

With respect to what publishers can do to make text mining easier, publishers
should:

1. provide XML if they have it
2. not change URL patterns so often, or at all
3. maintain consistent URL patterns among journals, years, etc.
4. keep their Crossref metadata up to date
5. open up their citation data

# Acknowledgments

This manuscript was greatly improved by comments from Maëlle Salmon.

# Data Accessibility

All scripts and data used in this paper can be found in the permanent
data archive Zenodo under the digital object identifier (DOI). This DOI
corresponds to a snapshot of the GitHub repository at
<https://github.com/ropensci/textmine>. Software can be found at
<https://github.com/ropensci>, all under MIT licenses.


# References

[rcrossref]: https://github.com/ropensci/rcrossref
[rorcid]: https://github.com/ropensci/rorcid
[fulltext]: https://github.com/ropensci/fulltext
[microdemic]: https://github.com/ropensci/microdemic
[arXiv]: https://github.com/ropensci/arXiv
[rdatacite]: https://github.com/ropensci/rdatacite
[rbhl]: https://github.com/ropensci/rbhl
[rentrez]: https://github.com/ropensci/rentrez
[rplos]: https://github.com/ropensci/rplos
[crminer]: https://github.com/ropensci/crminer
[europepmc]: https://github.com/ropensci/europepmc
[jaod]: https://github.com/ropensci/jaod
[jstor]: https://github.com/ropensci/jstor
[rcitoid]: https://github.com/ropenscilabs/rcitoid
[citecorp]: https://github.com/ropenscilabs/citecorp
[serrano]: https://github.com/sckott/serrano
[habanero]: https://github.com/sckott/habanero
[quanteda]: https://quanteda.io/
[pubchunks]: https://github.com/ropensci/pubchunks
[pdftools]: https://github.com/ropensci/pdftools
[libpoppler]: https://poppler.freedesktop.org/
[tesseract]: https://github.com/ropensci/tesseract
[rcoreoa]: https://github.com/ropensci/rcoreoa
