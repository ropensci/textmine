\documentclass[author-year, review, 11pt]{components/elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}
\usepackage{lineno} % add
  \linenumbers % turns line numbering on
\bibliographystyle{elsarticle-harv}
\biboptions{sort&compress} % For natbib
\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%% Redefines the elsarticle footer
\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{\it \hfill\today}%
 \let\@evenfoot\@oddfoot}
 \def\tightlist{}
\makeatother

% A modified page layout
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={R tools for accessing research literature for text mining},
            colorlinks=true,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header



\begin{document}
\begin{frontmatter}

  \title{R tools for accessing research literature for text mining}
    \author[cstar]{Scott Chamberlain\corref{c1}}
   \ead{myrmecocystus(at)gmail.com} 
   \cortext[c1]{Corresponding author}
      \address[cstar]{rOpenSci, Museum of Paleontology, University of California, Berkeley,
CA, USA}
  
  \begin{abstract}
  Text mining is a powerful method for answering research questions.
  However, getting texts to extract information can be a daunting and
  complicated task. The primary reason for this is the diversity of
  publisher technologies. There are thousands of different publishers,
  each with their own licenses, URL patterns, access options, and more.
  Layered on top of that is the varied access each user has based on their
  institutional affiliation. Here, I introduce a suite of software
  packages in the R programming language for fetching texts. The tapestry
  of different publishers, access levels, and other factors requires a
  patchwork of approaches for getting texts to users. The flagship R
  package called fulltext attempts to simplify search and retrieval of
  texts for text mining by serving as an interface to the varied and
  complex publishers. The fulltext package, along with many others, make
  acquiring texts easier than ever, facilitating answering research
  questions with text mining.
  \end{abstract}
  
 \end{frontmatter}


\newpage

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

There's more than 100 million research articles published (Crossref API:
\url{https://github.com/CrossRef/rest-api-doc}), representing an
enormous amount of knowledge. In addition to simply reading these
articles, the articles contain a vast trove of information of interest
to researchers for machine aided questions (Kong \& Gerstein, 2018; Usai
et al., 2018). For example, many researchers are interested in
statistical outcomes of articles that can be extracted from numeric
results: P-values, effect sizes, means, and more. In addition,
researchers are often interested in words in articles, their use through
time, and the contexts they are found in.

Text mining is the broad term associated with pulling information out of
articles. Given the importance of text mining, good text mining tools
are needed to make it easier for researchers to do. Graphical user
interface (GUI) based text mining tools are available (e.g., Ba \&
Bossy, 2016; Cañada et al., 2017; Muñoz, Kissling \& Loon, 2019) and
some research papers have used them (Chaix et al., 2019), but given the
urgent recent call to action for more reproducible research (Open
Science Collaboration, 2015; Camerer et al., 2016, 2018), we must move
away from GUI based tools as fast as possible. A number of examples of
programmatic tools can be found in the literature. For example, Sinclair
et al. (2016) present a tool in Python called seqenv for the domain
specific task of linking sequences to environments through text mining.

Most recent text mining papers do not use programmatic approaches,
highlighting the need for more programmatic text mining tools, and
increased discussion of those tools to increase awareness. For example,
many papers search Web of Science using their web interface, and
downloading papers manually (Ding, Li \& Fan, 2018; McCallen et al.,
2019). Many of these papers doing GUI based searching and paper
downloads are using R or Python downstream for analysis; replacing GUI
based data acquisition with programmatic approaches will improve
research.

The R programming language is free of cost, and is used widely
throughout many academic fields; tools in R for text mining are of
particular importance because they can be adopted by academics rapidly.

Here, I present an overview of text mining tools in the R programming
language, not for text mining analysis, but rather those tools for
searching for, acquiring, and extracting parts of texts (e.g., title,
abstract, authors). Most of the packages presented here are part of the
rOpenSci suite (\url{https://ropensci.org/}).

\hypertarget{digital-articles-technical-aspects}{%
\section{Digital articles: technical
aspects}\label{digital-articles-technical-aspects}}

Those articles that are digital (which in theory includes all articles)
can be split into two groups: machine readable and non-machine readable.

The machine readable articles are those in XML\footnote{\url{https://www.w3.org/TR/xml/}},
JSON\footnote{\url{https://tools.ietf.org/html/rfc7159}}, or plain text
format. The former two, XML and JSON, are the best machine readable
types because they are structured data\footnote{\url{https://en.wikipedia.org/wiki/Data_model}},
whereas plain text has no structure - it's simply a set of characters
with line breaks and spaces in between.

Of the non-machine readable types, the most noteable is the Portable
Document Format (PDF)\footnote{\url{https://en.wikipedia.org/wiki/PDF}}.
These can be broken out into two groups: text based PDFs and scanned
PDFs (images of text). The former are converted from digital versions of
various kinds (MS Word, OpenOffice, LaTeX, markdown, etc.), while the
latter are created by scanning print articles to a PDF format.
Text-based PDFs are much better for text mining purposes as plain text
can be extracted easily in R with
\href{https://github.com/ropensci/pdftools}{pdftools}, a binding to
\href{https://poppler.freedesktop.org/}{libpoppler}. However, with
scanned PDFs, text must be extracted using Optimal Character Recognition
(OCR; see R package
\href{https://github.com/ropensci/tesseract}{tesseract}), which isn't
always a clean solution, especially compared to true text based PDFs.

The reality in scholarly publishing is all publishers, if they provide
any access to their articles, only provide PDF format. Very few
publishers, with some quite large (Elsevier, Pensoft, PLOS), provide XML
format. Although most publishers most likely have the XML behind each of
their articles, they for some indefensible reason do not share it -
making text mining more difficult. Some provide plain text (Elsevier). I
only know of one publisher that provides full text as JSON (PLOS). Thus,
text mining, in most cases, will require extracting text from PDFs.

\hypertarget{digital-articles-the-access-landscape}{%
\section{Digital articles: the access
landscape}\label{digital-articles-the-access-landscape}}

Acces to full-text is the holy grail in text mining. Some use cases can
get by with article metadata (authors, title, etc.), some with
abstracts, but many use cases require full-text.

The landscape of access to full-text is extremely hetergeous, with the
majority of variation along the publisher axis. The major hurdle is
paywalls. The majority of articles are published by the big three
publishers - Wiley, Springer, Elsevier - and the majority of their
articles are behind paywalls.

A promising sign is an increasing number of open access articles, yet
open access articles represent a small percent of all articles: an
estimate in 2018 said that 28\% of the scholarly literature was open
access (Piwowar et al., 2018).

With respect to paywalled articles, access varies by institution,
depending on each institution's publisher contracts. MORE ABOUT THIS
\ldots{}

Some may not realize access to articles varies with IP address so that
access from campus vs.~from home (if not on a VPN) will drastically
differ. Sometimes a VPN is required, and this can provide a significant
technical hurdle to users attempting to do text mining work.

One final hurdle in text mining comes unsurprisingly from Elsevier. They
use so-called ``fences'' for programmatic access. That is, even if a
person trying to get an article programmatically their institution has
access to and they have access to, and they are on the correct IP
address, they may still not get access to an Elsevier article. Elsevier
puts in place these fences and only if you contact their technical team
directly can you get these fences removed, and only then on a per
institution basis.

I can not end this section without mentioning SciHub. This is a last
resort option for many probably (or possibly first, depending on your
level of access), providing access to full text of articles that are
normally paywalled. No tools in this manuscript provide access to
SciHub.

\hypertarget{the-discovery-problem}{%
\section{The discovery problem}\label{the-discovery-problem}}

A text mining project starts with a question. From that question,
researchers then attempt to acquire scholarly articles for text mining.
Finding appropriate articles is not altogether straight-forward.

Some of the discovery difficulty relates to the fact that there are so
many places to search for articles; a non-exhaustive list: Google
Scholar, Microsoft Academic Research, Scopus, ScienceDirect, Web of
Science, Pubmed/Entrez, Europe PMC, Directory of Open Access Journals,
Open Knowledge Maps, CORE, Fatcat, and more. It's probably difficult to
know where the best place is to search. Some of these are paywalled
(e.g., Web of Science), and some are not.

The most important aspect about any source for article search with
respect to reproducible research is being able to use the data source
programmatically. Of those listed above, the following can be used
programmatically: Microsoft Academic Research, Scopus, ScienceDirect,
Pubmed/Entrez, Europe PMC, and Directory of Open Access Journals. All of
these are included in the R package
\href{https://github.com/ropensci/fulltext}{fulltext}, discussed further
below.

On top of the vast array of different data sources is the varied ways
that search is implemented in each source. Most sources are probably
using Solr or Elasticsearch under the hood, though we can't know this
for sure as most do not make their software infrastructure public
knowledge. Nonetheless, data sources differ in how search works from the
user perspective. For example, some provide wild card/fuzzy search
(i.e., `appl*' includes results for `apple', `application', etc.) and
some do not. Some sources are searching full text of articles, while
others only search metadata (i.e., title, authors, abstract). In
addition, each source has a different set of metadata/full text
available. In brief, the same search against different sources produces
different results. Some text mining research articles perform the same
search against many different sources (refs), while others choose just
one source.

\hypertarget{data-sources}{%
\section{Data sources}\label{data-sources}}

There is increasing open access scientific literature content available
online. However, only a small proportion of scientific journals provide
access to their full text; whereas, most publishers provide open access
to their metadata only (most often through Crossref; Table 1). The
following is a synopsis of the major data sources and associated R
tools.

\newpage

Table 1. Sources of scientific literature, their content type provided
via web services, whether rOpenSci has an R packages for the service,
and where to find the API documentation.

\begin{longtable}[]{@{}llll@{}}
\toprule
Data Provider & Content Type & rOpenSci Package &
Documentation\tabularnewline
\midrule
\endhead
Crossref & Metadata & rcrossref/crminer & \footnote{\url{https://api.crossref.org}}\tabularnewline
DataCite & Metadata & rdatacite & \footnote{\url{https://support.datacite.org/docs/api}}\tabularnewline
Biodiversity Heritage Library & Full text/Metadata & rbhl & \footnote{\url{http://bit.ly/KYQ1Rd}}\tabularnewline
Public Library of Science (PLoS) & Full text/altmetrics & rplos &
\footnote{\url{http://api.plos.org/solr}}\tabularnewline
Scopus (Elsevier) & Full text/Metadata & fulltext & \footnote{\url{http://bit.ly/J9S616}}\tabularnewline
arXiv & Full text/Metadata & aRxiv & \footnote{\url{https://arxiv.org/help/api/index}}\tabularnewline
Biomed Central (via Springer) & Full text/Metadata & fulltext &
\footnote{\url{https://dev.springer.com/}}\tabularnewline
bioRxiv & Full text/Metadata & fulltext & \footnote{\url{http://www.biorxiv.org/}}\tabularnewline
PMC/Pubmed (via Entrez) & Full text/Metadata & rentrez & \footnote{\url{https://www.ncbi.nlm.nih.gov/books/NBK25500}}\tabularnewline
Europe PMC & Full text/Metadata & europepmc & \footnote{\url{https://azure.microsoft.com/en-us/services/cognitive-services}}\tabularnewline
Microsoft Academic Search & Metadata & microdemic & \footnote{\url{https://dev.labs.cognitive.microsoft.com/docs/services/56332331778daf02acc0a50b/operations/565d9001ca73072048922d97}}\tabularnewline
Directory of Open Access Journals & Metadata & jaod & \footnote{\url{https://doaj.org/api/v1/docs}}\tabularnewline
JSTOR Data for Research & Full text & jstor & \footnote{\url{https://www.jstor.org/dfr/}}\tabularnewline
ORCID & Metadata & rorcid & \footnote{\url{https://pub.orcid.org/}}\tabularnewline
Wikimedia's Citoid & Citations & rcitoid & \footnote{\url{https://en.wikipedia.org/api/rest_v1/\#/Citation/getCitation}}\tabularnewline
Open Citation Corpus & Citations & citecorp & \footnote{\url{http://opencitations.net/}}\tabularnewline
Fatcat & Metadata & none & \footnote{\url{https://fatcat.wiki/}}\tabularnewline
SHERPA/RoMEO & Journal Level Metadata & rromeo & \footnote{\url{http://www.sherpa.ac.uk/romeo/apimanual.php?la=en\&fIDnum=\%7C\&mode=simple}}\tabularnewline
CORE & Full text/Metadata & rcoreoa & \footnote{\url{https://core.ac.uk/}}\tabularnewline
Dissemin & Metadata & dissemr & \footnote{\url{https://dissemin.readthedocs.io/en/latest/api.html}}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{crossref-and-datacite}{%
\subsection{Crossref and Datacite}\label{crossref-and-datacite}}

Crossref is a non-profit that creates (or ``mints'') Digital Object
Identifiers (DOIs). In addition, they maintain metadata associated with
each DOI. The metadata ranges from simple (including author, title,
dates, DOI, type, publisher) to including number of citations to the
article, as well as references in the article, and even abstracts. At
the time of writing they hold 100 million DOIs.

One can search by DOI or search citation data to get citations. In
addition, Crossref has a text mining opt-in program for publishers. The
result of this is that some publishers provide URLs for full text
content of their articles. The majority of these links are pay-walled,
while some are open access. Using any of the various tools for working
with Crossref data, you can filter your search to get only articles with
full text links, and further to get only articles with full text links
that are open access.

The main interfaces for Crossref in R are
\href{https://github.com/ropensci/rcrossref}{rcrossref} and
\href{https://github.com/ropensci/crminer}{crminer}. rcrossref is a
complete client for the public facing Crossref web services including
metadata, whereas crminer focuses only on retrieving full text of
articles. Similar interfaces to rcrossref are available in Ruby
(\href{https://github.com/sckott/serrano}{serrano}) and Python
(\href{https://github.com/sckott/habanero}{habanero}).

Datacite is similar to Crossref, but focuses on datasets instead of
articles. The main interface for Datacite in R is
\href{https://github.com/ropensci/rdatacite}{rdatacite}.

\hypertarget{biodiversity-heritage-library}{%
\subsection{Biodiversity Heritage
Library}\label{biodiversity-heritage-library}}

The Biodiversity Heritage Library (BHL) houses scans of biodiversity
books, and provides web interfaces and APIs to query and fetch those
data. They also provide text of the scanned pages. The main R interace
to BHL is through \href{https://github.com/ropensci/rbhl}{rbhl}.

\hypertarget{public-library-of-science}{%
\subsection{Public Library of Science}\label{public-library-of-science}}

The Public Library of Science (PLOS) is one of the largest open access
only publishers. They as of this writing have published 2.1 million
articles. One of the strongs advantages of PLOS is that they provide an
API to their Solr instance, which is a very flexible way to search their
articles. The main R interace to PLOS is through
\href{https://github.com/ropensci/rplos}{rplos}.

\hypertarget{elsevierscopus}{%
\subsection{Elsevier/Scopus}\label{elsevierscopus}}

Elsevier is one of the largest publishers. Most of their articles are
not open access. However, they have a numbrer of advantages if you have
access to their articles: they are one of the few publishers to provide
machine readable XML (many publishers do have XML versions of articles,
but do not provide it); they are one of the few (two) publishers part of
Crossref's text and data mining program. The packages
\href{https://github.com/ropensci/fulltext}{fulltext} and
\href{https://github.com/ropensci/crminer}{crminer} can be used to
access Elsevier articles through Crossref's TDM program. There's an
interface to Scopus article search within
\href{https://github.com/ropensci/fulltext}{fulltext}.

\hypertarget{arxivbiorxiv}{%
\subsection{arXiv/bioRxiv}\label{arxivbiorxiv}}

arXiv and bioRxiv are preprint publishers, the former in existence for
many years, and the latter new on the scene. You can access articles
from these publishers through
\href{https://github.com/ropensci/fulltext}{fulltext}. arXiv does
provide a web API that we hook into; bioRxiv does not, but we can get
you articles nonetheless.

\hypertarget{pubmedpmceurope-pmc}{%
\subsection{Pubmed/PMC/Europe PMC}\label{pubmedpmceurope-pmc}}

Pubmed/PMC is a corpus/website of NIH funded research in the United
States; while Europe PMC is an equivalent for the European Union. You
can access articles from Pubmed/PMC through
\href{https://github.com/ropensci/fulltext}{fulltext}, and for Europe
PMC through \href{https://github.com/ropensci/europepmc}{europepmc}.

\hypertarget{microsoft-academic-research}{%
\subsection{Microsoft Academic
Research}\label{microsoft-academic-research}}

Microsoft Academic Research (MAR) is a search engine for research
articles. You can use their GUI web interface to search, and they
provide APIs for programmatic access. The R interface for MAR is
\href{https://github.com/ropensci/microdemic}{microdemic}; and
\href{https://github.com/ropensci/fulltext}{fulltext} hooks into
\texttt{microdemic} as well for article search and abstract retrieval.

\hypertarget{directory-of-open-access-journals}{%
\subsection{Directory of Open Access
Journals}\label{directory-of-open-access-journals}}

Directory of Open Access Journals (DOAJ) maintains data on open access
journals, as well as some portion of the articles in those journals.
Thus, you can search for journals as well as articles with DOAJ. The R
interface for DOAJ is \href{https://github.com/ropensci/jaod}{jaod}.

\hypertarget{jstor}{%
\subsection{JSTOR}\label{jstor}}

JSTOR's Data for Research program gives institutions with access to
JSTOR, access to full text of articles within JSTOR. There is no way
however to make the interaction with JSTOR completely programmatic, thus
making reproducible research very difficult. Nonetheless, there is an R
package (\href{https://github.com/ropensci/jstor}{jstor}) for using data
from JSTOR's Data for Research.

\hypertarget{orcid}{%
\subsection{ORCID}\label{orcid}}

ORCID (\url{https://orcid.org/}) is an organization keeping track of
identifiers and metadata for researchers around the world. Individuals
can optionally maintain metadata on their scholarly works connected to
their account with ORCID. Thus, across all of ORCID, a significant cache
of metadata is accruing on scholarly works, their funding amounts,
collaborators, etc., useful for bibliometrics research and more. The R
interface for ORCID is
\href{https://github.com/ropensci/rorcid}{rorcid}.

\hypertarget{citoidopen-citation-corpus}{%
\subsection{Citoid/Open Citation
Corpus}\label{citoidopen-citation-corpus}}

The Open Citation Corpus (\url{http://opencitations.net/}) holds records
of which articles cite which other articles, allowing for all important
research on the scholarly web of citation. Citation data has been very
closely guarded until recently, but the largest publishers are still not
contributing to the Open Citation Corpus. The R interface to the Open
Citation Corpus is
\href{https://github.com/ropenscilabs/rcitoid}{rcitoid}.

\hypertarget{fatcat}{%
\subsection{Fatcat}\label{fatcat}}

Fatcat is a project from Ben Newbold of the Internet Archive Labs. It is
a ``versioned, publicly-editable catalog of research publications:
journal articles, conference proceedings, pre-prints, blog posts''.
Fatcat is currently does not have an R client, but is used inside of the
\href{https://github.com/ropensci/fulltext}{fulltext} package.

\hypertarget{sherparomeo}{%
\subsection{SHERPA/RoMEO}\label{sherparomeo}}

SHERPA/RoMEO (\url{http://sherpa.mimas.ac.uk/romeo/index.php})
aggregates and analyses publisher open access policies and provides
summaries of self-archiving permissions and conditions of rights given
to authors. The {[}rromeo{]}{[}{]} is an R interface to SHERPA/RoMEO.

\hypertarget{core}{%
\subsection{CORE}\label{core}}

CORE (\url{https://core.ac.uk/}) touts itself as the world's largest
collection of open access research articles, providing metadata on
journals and articles, as well as access to the full text of articles.
The \href{https://github.com/ropensci/rcoreoa}{rcoreoa} R package
interfaces with the CORE API.

\hypertarget{dissemin}{%
\subsection{Dissemin}\label{dissemin}}

Dissemin (\url{https://dissem.in/}) detects papers behind pay-walls and
invites authors to upload them to an open repository. Dissemin provides
metadata including links to open versions of articles. The
{[}dissemr{]}{[}{]} R package interfaces with the Dissemin API.

\hypertarget{fulltext-a-toolset-for-text-mining-in-r}{%
\section{fulltext: a toolset for text mining in
R}\label{fulltext-a-toolset-for-text-mining-in-r}}

\href{https://github.com/ropensci/fulltext}{fulltext} is a general
purpose R package for the data part of text mining: search for articles,
get links to articles, get article abstracts, and fetch full text of
articles. The \texttt{fulltext} package is always adding additional data
sources as time allows (See Table 1). Starting from searching for
articles, the outputs of search can be fed into a function to get links
to those articles, or to get abstracts for those articles, or to fetch
their full text. The following is a breakdown of the major distinct
parts of \texttt{fulltext}.

\hypertarget{search}{%
\subsection{Search}\label{search}}

\texttt{ft\_search()} provides search access to nine different data
sources (PLOS, BMC, Crossref, Entrez, arXiv, bioRxiv, Europe PMC,
Scopus, Microsoft Academic), creating a mostly unified interface to all
data sources. The parts of each data source that are common are for the
most part factored out into the parameters of the \texttt{ft\_search()}
function: query term(s), pagination (number of results, result number to
start at). In addition, we allow the user to pass on data source
specific options to refine the search per data source.

With \texttt{ft\_search()}, you can query any combination of the nine
data sources at once. The returned object is a list, with access to
results of each data source by its name (e.g., \texttt{\$plos}, or
\texttt{\$crossref}). For each data source, the returned object does
vary because the returned data from each data source widely varies; for
the most part data.frame's are returned. For those data sources not
queried, their slot is empty.

One important aspect of the research result we highlight is the licenses
in the returned data for each data source.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{ft_search}\NormalTok{(}\DataTypeTok{query =} \StringTok{'ecology'}\NormalTok{, }\DataTypeTok{from =} \KeywordTok{c}\NormalTok{(}\StringTok{"plos"}\NormalTok{, }\StringTok{"crossref"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The results for this PLOS search have all CC-BY licenses

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{$}\NormalTok{plos}
\CommentTok{#> Query: [ecology] }
\CommentTok{#> Records found, returned: [47257, 10] }
\CommentTok{#> License: [CC-BY] }
\CommentTok{#>                              id}
\CommentTok{#> 1  10.1371/journal.pone.0001248}
\CommentTok{#> 2  10.1371/journal.pone.0059813}
\CommentTok{#> 3  10.1371/journal.pone.0080763}
\CommentTok{#> 4  10.1371/journal.pone.0155019}
\CommentTok{#> 5  10.1371/journal.pone.0175014}
\CommentTok{#> 6  10.1371/journal.pone.0150648}
\CommentTok{#> 7  10.1371/journal.pone.0208370}
\CommentTok{#> 8  10.1371/journal.pcbi.1003594}
\CommentTok{#> 9  10.1371/journal.pone.0102437}
\CommentTok{#> 10 10.1371/journal.pone.0166559}
\end{Highlighting}
\end{Shaded}

Whereas the results for this Crossref search have mixed licenses

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{$}\NormalTok{crossref}
\CommentTok{#> Query: [ecology] }
\CommentTok{#> Records found, returned: [164657, 10] }
\CommentTok{#> License: [variable, see individual records] }
\CommentTok{#>    archive               container.title    created  deposited}
\CommentTok{#> 1  Portico                       Ecology 2006-05-03 2018-08-04}
\CommentTok{#> 2  Portico                       Ecology 2006-05-03 2018-08-04}
\CommentTok{#> 3       NA                       Ecology 2006-05-03 2018-08-04}
\CommentTok{#> 4       NA                       Ecology 2006-05-03 2018-08-04}
\CommentTok{#> 5       NA                       Ecology 2006-05-03 2018-08-04}
\CommentTok{#> 6       NA                       Ecology 2006-05-03 2018-08-04}
\CommentTok{#> 7       NA                       Ecology 2006-05-09 2018-08-01}
\CommentTok{#> 8  Portico                       Ecology 2017-04-26 2019-03-08}
\CommentTok{#> 9       NA Trends in Ecology & Evolution 2002-07-25 2017-06-14}
\CommentTok{#> 10      NA Journal of Industrial Ecology 2014-11-21 2017-06-23}
\CommentTok{#> Variables not shown: published.print (chr), published.online (chr), doi}
\CommentTok{#>      (chr), indexed (chr), issn (chr), issue (chr), issued (chr), member}
\CommentTok{#>      (chr), page (chr), prefix (chr), publisher (chr), reference.count}
\CommentTok{#>      (chr), score (chr), source (chr), title (chr), type (chr), url (chr),}
\CommentTok{#>      volume (chr), author (list), link (list), license (list), subject}
\CommentTok{#>      (chr), alternative.id (chr), subtitle (chr), reference (list)}
\end{Highlighting}
\end{Shaded}

You can dig into the license field for each article, with URLs holding
information on each license

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vapply}\NormalTok{(x}\OperatorTok{$}\NormalTok{crossref}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\NormalTok{license, }\ControlFlowTok{function}\NormalTok{(w) w}\OperatorTok{$}\NormalTok{URL[}\DecValTok{1}\NormalTok{], }\StringTok{""}\NormalTok{)}
\CommentTok{#>  [1] "http://doi.wiley.com/10.1002/tdm_license_1.1"}
\CommentTok{#>  [2] "http://doi.wiley.com/10.1002/tdm_license_1.1"}
\CommentTok{#>  [3] "http://doi.wiley.com/10.1002/tdm_license_1"  }
\CommentTok{#>  [4] "http://doi.wiley.com/10.1002/tdm_license_1.1"}
\CommentTok{#>  [5] "http://doi.wiley.com/10.1002/tdm_license_1"  }
\CommentTok{#>  [6] "http://doi.wiley.com/10.1002/tdm_license_1"  }
\CommentTok{#>  [7] "http://doi.wiley.com/10.1002/tdm_license_1"  }
\CommentTok{#>  [8] "http://doi.wiley.com/10.1002/tdm_license_1.1"}
\CommentTok{#>  [9] "http://www.elsevier.com/tdm/userlicense/1.0/"}
\CommentTok{#> [10] "http://doi.wiley.com/10.1002/tdm_license_1.1"}
\end{Highlighting}
\end{Shaded}

\hypertarget{links}{%
\subsection{Links}\label{links}}

\texttt{ft\_links()} provides two pathways to get links (URLs) for
articles, with a choice of four different data sources (PLOS, BMC,
Crossref, Entrez). First, you can use \texttt{ft\_search()}, then pass
the output of that function to \texttt{ft\_links()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{out <-}\StringTok{ }\KeywordTok{ft_search}\NormalTok{(}\DataTypeTok{query =} \StringTok{"ecology"}\NormalTok{, }\DataTypeTok{from =} \StringTok{"entrez"}\NormalTok{)}
\KeywordTok{ft_links}\NormalTok{(out)}
\CommentTok{#> <fulltext links>}
\CommentTok{#> [Found] 6 }
\CommentTok{#> [IDs] ID_30964001 ID_30962485 ID_30962432 ID_30952928 ID_30674747}
\CommentTok{#>      ID_30674743 ...}
\end{Highlighting}
\end{Shaded}

Second, you can pass DOIs directly to \texttt{ft\_links()}. Both end up
at the same point, links for each article, if they could be found for
the user selected data source.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# }\AlertTok{FIXME}
\KeywordTok{ft_links}\NormalTok{(out}\OperatorTok{$}\NormalTok{entrez}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\NormalTok{doi)}
\end{Highlighting}
\end{Shaded}

The biggest caveat with \texttt{ft\_links()} is that we can't gaurantee
that the links will work. Link rot is one way in which the links may not
work: link rot is when the URL does not point to the original content
anymore, or fails altogether. Additionally, with Crossref, publishers
can deposit URLs for articles, but they make change the URLs at some
later date but not update the URLs with Crossref.

\hypertarget{abstracts}{%
\subsection{Abstracts}\label{abstracts}}

\texttt{ft\_abstract()} provides access to article abstracts from four
different data sources (PLOS, Scopus, Microsoft Academic Research,
Crossref). The only way to use the function is to pass article
identifiers, which are for the most DOIs.

The advantage of abstracts over full text is that abstracts can often be
retrieved even for paywalled articles. That is, you can have much
broader coverage of the articles you're targeting relative to full text.

If you are after abstracts, and you are already getting or already have
full text, and if the articles are in XML format, then you can use
\href{https://github.com/ropensci/pubchunks}{pubchunks} to extract out
the abstracts.

\hypertarget{fetch-full-text}{%
\subsection{Fetch full text}\label{fetch-full-text}}

\texttt{ft\_get()} fetchs full text of articles from many different data
sources. From the DOIs that are passed in to the function, we detect the
publisher, and there are specific plugins for certain publishers: AAAS,
American Institute of Physics, American Society of Clinical Oncology,
American Society for Microbiology, arXiv, bioRxiv, BiomedCentral,
Copernicus, Crossref, Elife, Elsevier, Pubmed/PMC via NCBI's Entrez,
Frontiers, IEEE, Informa, Instituto de Investigaciones Filologicas,
American Medical Association, Microbiology Society, PeerJ, Pensoft,
PLOS, PNAS, Royal Society of Chemistry, ScienceDirect, Scientific
Societies, and Wiley.

If there's no built-in plugin for the publisher already, we use the
FTDOI API (\url{https://ftdoi.org}) to try to get the link for the full
text of the article. If the FTDOI API doesn't bear fruit, we search
Crossref for a link to the full text. If Crossref doesn't have any full
text links, we give up.

Since users can go through a lot of article requests, we cache
successfully downloaded articles, and keep that knowledge consistent
across R sessions; all subsequent requests for the same article just use
the cached version. Additionally, all errors in \texttt{ft\_get()} are
collected in a tidy data.frame in the output of the function to help the
user quickly determine what went wrong.

\hypertarget{how-to-text-mine-from-r-three-case-studies}{%
\section{How to text mine from R: Three case
studies}\label{how-to-text-mine-from-r-three-case-studies}}

\hypertarget{case-study-1-citation-mining}{%
\subsection{Case study 1: Citation
mining}\label{case-study-1-citation-mining}}

In this example, xxxx

\emph{Load libraries}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"rcrossref"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rplos"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rorcid"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"rcitoid"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(}\StringTok{"citecorp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\emph{rcrossref}

Using \texttt{rcrossref} for Crossref data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{cr_works}\NormalTok{(}\DataTypeTok{query=}\StringTok{"NSF"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(x}\OperatorTok{$}\NormalTok{data)}
\CommentTok{#> # A tibble: 6 x 32}
\CommentTok{#>   alternative.id container.title created deposited published.print doi  }
\CommentTok{#>   <chr>          <chr>           <chr>   <chr>     <chr>           <chr>}
\CommentTok{#> 1 S106352031630~ Applied and Co~ 2016-0~ 2019-02-~ 2018-03         10.1~}
\CommentTok{#> 2 <NA>           Biogeosciences~ 2017-0~ 2017-07-~ <NA>            10.5~}
\CommentTok{#> 3 <NA>           Global Biogeoc~ 2018-0~ 2019-01-~ 2018-10         10.1~}
\CommentTok{#> 4 <NA>           IEEE Communica~ 2016-1~ 2017-12-~ 2017            10.1~}
\CommentTok{#> 5 S002178241400~ Journal de Mat~ 2014-0~ 2018-10-~ 2014-10         10.1~}
\CommentTok{#> 6 123            Light: Science~ 2019-0~ 2019-01-~ 2019-12         10.1~}
\CommentTok{#> # ... with 26 more variables: indexed <chr>, issn <chr>, issue <chr>,}
\CommentTok{#> #   issued <chr>, member <chr>, page <chr>, prefix <chr>, publisher <chr>,}
\CommentTok{#> #   reference.count <chr>, score <chr>, ...}
\end{Highlighting}
\end{Shaded}

\hypertarget{case-study-2-abstract-mining}{%
\subsection{Case study 2: Abstract
mining}\label{case-study-2-abstract-mining}}

Sometimes you just need abstracts for your research question. The
benefit of only needing abstracts, and not need full text, is that
there's many more articles that will have abstracts available than have
their full text available.

As an example, let's say you xxxx

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"fulltext"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\emph{xxxxx}

Using \texttt{fulltext}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res <-}\StringTok{ }\KeywordTok{ft_search}\NormalTok{(}\StringTok{"ecology"}\NormalTok{, }\DataTypeTok{from =} \StringTok{"crossref"}\NormalTok{,}
  \DataTypeTok{crossrefopts =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{filter =} \KeywordTok{c}\NormalTok{(}\DataTypeTok{has_abstract =} \OtherTok{TRUE}\NormalTok{)))}
\NormalTok{ids <-}\StringTok{ }\NormalTok{res}\OperatorTok{$}\NormalTok{crossref}\OperatorTok{$}\NormalTok{data}\OperatorTok{$}\NormalTok{doi}
\NormalTok{out <-}\StringTok{ }\KeywordTok{ft_abstract}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ ids, }\DataTypeTok{from =} \StringTok{"crossref"}\NormalTok{)}
\NormalTok{abstracts <-}\StringTok{ }\KeywordTok{vapply}\NormalTok{(out}\OperatorTok{$}\NormalTok{crossref, }\StringTok{"[["}\NormalTok{, }\StringTok{""}\NormalTok{, }\StringTok{"abstract"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Using \href{https://quanteda.io/}{quanteda}, read the abstracts into a
corpus

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"quanteda"}\NormalTok{)}
\NormalTok{corp <-}\StringTok{ }\KeywordTok{corpus}\NormalTok{(abstracts)}
\KeywordTok{docvars}\NormalTok{(corp) <-}\StringTok{ }\NormalTok{ids}
\end{Highlighting}
\end{Shaded}

Get a summary of the abstracts

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(corp)}
\CommentTok{#> Corpus consisting of 10 documents:}
\CommentTok{#> }
\CommentTok{#>    Text Types Tokens Sentences                    V1}
\CommentTok{#>   text1   143    262        10   10.2458/v22i1.21112}
\CommentTok{#>   text2   117    244         6   10.2458/v17i1.21696}
\CommentTok{#>   text3    75    118         4   10.2458/v25i1.23119}
\CommentTok{#>   text4     5      8         1    10.2458/v1i1.21154}
\CommentTok{#>   text5   105    171         7   10.1155/2011/868426}
\CommentTok{#>   text6   112    181         6   10.1155/2012/273413}
\CommentTok{#>   text7   117    240         8 10.5194/we-13-91-2013}
\CommentTok{#>   text8   140    245         9 10.5194/we-13-95-2013}
\CommentTok{#>   text9   107    202         7   10.1155/2014/198707}
\CommentTok{#>  text10   118    224         6   10.5402/2011/897578}
\CommentTok{#> }
\CommentTok{#> Source: /Users/sckott/github/ropensci/textmine/use-cases/* on x86_64 by sckott}
\CommentTok{#> Created: Thu Apr 11 11:20:19 2019}
\CommentTok{#> Notes:}
\end{Highlighting}
\end{Shaded}

Use the \texttt{kwic()} function to see a word in context across the
abstracts

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{kwic}\NormalTok{(corp, }\DataTypeTok{pattern =} \StringTok{"ecology"}\NormalTok{)}
\CommentTok{#>                                                                         }
\CommentTok{#>   [text1, 33] knowledge production within critical political | ecology |}
\CommentTok{#>   [text1, 50]              in scientific articles on dryland | ecology |}
\CommentTok{#>  [text1, 204]                 to equilibrium models in range | ecology |}
\CommentTok{#>  [text1, 246]    communal areas.Keywords: Critical political | ecology |}
\CommentTok{#>  [text1, 255]                 , scientific models, rangeland | ecology |}
\CommentTok{#>    [text2, 5]                            < jats:p> Political | ecology |}
\CommentTok{#>   [text2, 23]        manifestations of political economy and | ecology |}
\CommentTok{#>   [text2, 45]                      I try to extend political | ecology |}
\CommentTok{#>  [text2, 149]                   , in dialogue with political | ecology |}
\CommentTok{#>  [text2, 177]            people and resources that political | ecology |}
\CommentTok{#>  [text2, 229]      indigeneity scholars.Key words: political | ecology |}
\CommentTok{#>   [text3, 71]                   an analysis from a political | ecology |}
\CommentTok{#>  [text3, 114]                system, supermarkets, political | ecology |}
\CommentTok{#>  [text6, 134]                was observed when allopatry and | ecology |}
\CommentTok{#>  [text7, 167]             ecosystem should be considered for | ecology |}
\CommentTok{#>  [text7, 185]                       the" four-color issue of | ecology |}
\CommentTok{#>  [text7, 201]             step toward advancing knowledge in | ecology |}
\CommentTok{#>  [text9, 195]         or for theoretical studies integrating | ecology |}
\CommentTok{#>                                              }
\CommentTok{#>  . This article is a                         }
\CommentTok{#>  , and investigates the functions            }
\CommentTok{#>  , and the fence-line photographs            }
\CommentTok{#>  , fence-line photography, scientific        }
\CommentTok{#>  , Southern Africa</                         }
\CommentTok{#>  has expanded in multiple new                }
\CommentTok{#>  in the" problem"                            }
\CommentTok{#>  to engage with ethnic studies               }
\CommentTok{#>  approaches to better understand the         }
\CommentTok{#>  focuses on cannot be adequately             }
\CommentTok{#>  , coloniality, Maidu,                       }
\CommentTok{#>  standpoint allows a different interpretation}
\CommentTok{#>  </ jats:p>                                  }
\CommentTok{#>  act together, leading to                    }
\CommentTok{#>  "? Here, I                                  }
\CommentTok{#>  ", and propose that                         }
\CommentTok{#>  and conservation biology. In                }
\CommentTok{#>  and biogeography.</}
\end{Highlighting}
\end{Shaded}

\hypertarget{case-study-3-full-text-mining}{%
\subsection{Case study 3: Full text
mining}\label{case-study-3-full-text-mining}}

In this example, xxxx

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"fulltext"}\NormalTok{)}
\CommentTok{# library("crminer")}
\end{Highlighting}
\end{Shaded}

\emph{Search for articles}

Search for the term \emph{ecology} in PLOS journals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(res1 <-}\StringTok{ }\KeywordTok{ft_search}\NormalTok{(}\DataTypeTok{query =} \StringTok{'ecology'}\NormalTok{, }\DataTypeTok{from =} \StringTok{'plos'}\NormalTok{))}
\CommentTok{#> Query:}
\CommentTok{#>   [ecology] }
\CommentTok{#> Found:}
\CommentTok{#>   [PLoS: 47272; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] }
\CommentTok{#> Returned:}
\CommentTok{#>   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0]}
\end{Highlighting}
\end{Shaded}

Each publisher/search-engine has a slot with metadata and data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res1}\OperatorTok{$}\NormalTok{plos}
\CommentTok{#> Query: [ecology] }
\CommentTok{#> Records found, returned: [47272, 10] }
\CommentTok{#> License: [CC-BY] }
\CommentTok{#>                              id}
\CommentTok{#> 1  10.1371/journal.pone.0001248}
\CommentTok{#> 2  10.1371/journal.pone.0059813}
\CommentTok{#> 3  10.1371/journal.pone.0080763}
\CommentTok{#> 4  10.1371/journal.pone.0155019}
\CommentTok{#> 5  10.1371/journal.pone.0175014}
\CommentTok{#> 6  10.1371/journal.pone.0150648}
\CommentTok{#> 7  10.1371/journal.pone.0208370}
\CommentTok{#> 8  10.1371/journal.pcbi.1003594}
\CommentTok{#> 9  10.1371/journal.pone.0102437}
\CommentTok{#> 10 10.1371/journal.pone.0166559}
\end{Highlighting}
\end{Shaded}

\emph{Get full text}

Using the results from \texttt{ft\_search()} we can grab full text of
some articles

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(out <-}\StringTok{ }\KeywordTok{ft_get}\NormalTok{(res1))}
\CommentTok{#> <fulltext text>}
\CommentTok{#> [Docs] 10 }
\CommentTok{#> [Source] ext - /Users/sckott/Library/Caches/R/fulltext }
\CommentTok{#> [IDs] 10.1371/journal.pone.0001248 10.1371/journal.pone.0059813}
\CommentTok{#>      10.1371/journal.pone.0080763 10.1371/journal.pone.0155019}
\CommentTok{#>      10.1371/journal.pone.0175014 10.1371/journal.pone.0150648}
\CommentTok{#>      10.1371/journal.pone.0208370 10.1371/journal.pcbi.1003594}
\CommentTok{#>      10.1371/journal.pone.0102437 10.1371/journal.pone.0166559 ...}
\end{Highlighting}
\end{Shaded}

\emph{Extract text from pdfs}

Ideally for text mining you have access to XML or other text based
formats. However, sometimes you only have access to PDFs. In this case
you want to extract text from PDFs. \texttt{fulltext} can help with
that.

You can extract from any pdf from a file path, like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{path <-}\StringTok{ }\KeywordTok{system.file}\NormalTok{(}\StringTok{"examples"}\NormalTok{, }\StringTok{"example1.pdf"}\NormalTok{, }\DataTypeTok{package =} \StringTok{"fulltext"}\NormalTok{)}
\KeywordTok{ft_extract}\NormalTok{(path)}
\CommentTok{#> <document>/Library/Frameworks/R.framework/Versions/3.5/Resources/library/fulltext/examples/example1.pdf}
\CommentTok{#>   Title: Suffering and mental health among older people living in nursing homes---a mixed-methods study}
\CommentTok{#>   Producer: pdfTeX-1.40.10}
\CommentTok{#>   Creation date: 2015-07-17}
\end{Highlighting}
\end{Shaded}

\emph{Extract text chunks}

Requires the \href{https://github.com/ropensci/pubchunks}{pubchunks}
library. Here, we'll search for some PLOS articles, then get their full
text, then extract various parts of each article with
\texttt{pub\_chunks()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"pubchunks"}\NormalTok{)}
\NormalTok{res <-}\StringTok{ }\KeywordTok{ft_search}\NormalTok{(}\DataTypeTok{query =} \StringTok{"ecology"}\NormalTok{, }\DataTypeTok{from =} \StringTok{"plos"}\NormalTok{, }\DataTypeTok{limit =} \DecValTok{3}\NormalTok{)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{ft_get}\NormalTok{(res)}
\NormalTok{x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ft_collect}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pub_chunks}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"doi"}\NormalTok{, }\StringTok{"history"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pub_tabularize}\NormalTok{()}
\CommentTok{#> $plos}
\CommentTok{#> $plos$`10.1371/journal.pone.0001248`}
\CommentTok{#>                            doi history.received history.accepted}
\CommentTok{#> 1 10.1371/journal.pone.0001248       2007-07-02       2007-11-06}
\CommentTok{#>   .publisher}
\CommentTok{#> 1       plos}
\CommentTok{#> }
\CommentTok{#> $plos$`10.1371/journal.pone.0059813`}
\CommentTok{#>                            doi history.received history.accepted}
\CommentTok{#> 1 10.1371/journal.pone.0059813       2012-09-16       2013-02-19}
\CommentTok{#>   .publisher}
\CommentTok{#> 1       plos}
\CommentTok{#> }
\CommentTok{#> $plos$`10.1371/journal.pone.0080763`}
\CommentTok{#>                            doi history.received history.accepted}
\CommentTok{#> 1 10.1371/journal.pone.0080763       2013-08-15       2013-10-16}
\CommentTok{#>   .publisher}
\CommentTok{#> 1       plos}
\end{Highlighting}
\end{Shaded}

\hypertarget{future-directions}{%
\section{Future directions}\label{future-directions}}

Text mining will always be a complex task given all the layers involved:
often temporal time-span of research questions; varied permissions among
researchers and their articles they're trying to access; varied
approaches to getting full text (xml vs pdf vs plain text); and more.

Programmatic text mining is a first step towards making text mining
easier. The R ecosystem is an especially good place to do text mining
because there are many packages for text mining analysis, and endless
packages for any required statistical analyses. In addition, rOpenSci
and others are building up a set of packages in R for searching for and
acquiring full text programatically to help make the research workflow
as reproducible as possible.

Future work for \texttt{fulltext} includes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adding more publisher plugins
\item
  Fine tuned user control over publishers
\item
  Improve VPN/proxy controls
\item
  Incorporate more search engines to help resolve URLs for fulltext
  versions
\item
  Improve documentation
\end{enumerate}

With respect to what publishers can do to make text mining easier,
publishers should:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  provide XML if they have it
\item
  not change URL patterns so often, or at all
\item
  maintain consistent URL patterns among journals, years, etc.
\item
  keep their Crossref metadata up to date
\item
  open up their citation data
\end{enumerate}

\hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}

xxxx

\hypertarget{data-accessibility}{%
\section{Data Accessibility}\label{data-accessibility}}

All scripts and data used in this paper can be found in the permanent
data archive Zenodo under the digital object identifier (DOI). This DOI
corresponds to a snapshot of the GitHub repository at
\url{https://github.com/ropensci/textmine}. Software can be found at
\url{https://github.com/ropensci/xxx}, xxxx, all under MIT licenses.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-ba2016interoperability}{}%
Ba M., Bossy R. 2016. Interoperability of corpus processing work-flow
engines: The case of alvisnlp/ml in openminted. In: \emph{Proceedings of
the workshop on cross-platform text mining and natural language
processing interoperability (interop 2016) at lrec}. 15--18.

\leavevmode\hypertarget{ref-Camerer2016}{}%
Camerer CF., Dreber A., Forsell E., Ho T-H., Huber J., Johannesson M.,
Kirchler M., Almenberg J., Altmejd A., Chan T., Heikensten E.,
Holzmeister F., Imai T., Isaksson S., Nave G., Pfeiffer T., Razen M., Wu
H. 2016. Evaluating replicability of laboratory experiments in
economics. \emph{Science} 351:1433--1436.

\leavevmode\hypertarget{ref-Camerer2018}{}%
Camerer CF., Dreber A., Holzmeister F., Ho T-H., Huber J., Johannesson
M., Kirchler M., Nave G., Nosek BA., Pfeiffer T., Altmejd A., Buttrick
N., Chan T., Chen Y., Forsell E., Gampa A., Heikensten E., Hummer L.,
Imai T., Isaksson S., Manfredi D., Rose J., Wagenmakers E-J., Wu H.
2018. Evaluating the replicability of social science experiments in
nature and science between 2010 and 2015. \emph{Nature Human Behaviour}
2:637--644.

\leavevmode\hypertarget{ref-Canada2017}{}%
Cañada A., Capella-Gutierrez S., Rabal O., Oyarzabal J., Valencia A.,
Krallinger M. 2017. LimTox: A web tool for applied text mining of
adverse event and toxicity associations of compounds, drugs and genes.
\emph{Nucleic Acids Research} 45:W484--W489.

\leavevmode\hypertarget{ref-Chaix2019}{}%
Chaix E., Deléger L., Bossy R., Nédellec C. 2019. Text mining tools for
extracting information about microbial biodiversity in food. \emph{Food
Microbiology} 81:63--75.

\leavevmode\hypertarget{ref-Ding2018}{}%
Ding Z., Li Z., Fan C. 2018. Building energy savings: Analysis of
research trends based on text mining. \emph{Automation in Construction}
96:398--410.

\leavevmode\hypertarget{ref-Kong2018}{}%
Kong X., Gerstein MB. 2018. Text mining systems biology: Turning the
microscope back on the observer. \emph{Current Opinion in Systems
Biology} 11:117--122.

\leavevmode\hypertarget{ref-McCallen2019}{}%
McCallen E., Knott J., Nunez-Mir G., Taylor B., Jo I., Fei S. 2019.
Trends in ecology: Shifts in ecological research themes over the past
four decades. \emph{Frontiers in Ecology and the Environment}
17:109--116.

\leavevmode\hypertarget{ref-Munoz2019}{}%
Muñoz G., Kissling WD., Loon EE van. 2019. Biodiversity observations
miner: A web application to unlock primary biodiversity data from
published literature. \emph{Biodiversity Data Journal} 7.

\leavevmode\hypertarget{ref-OSC2015}{}%
Open Science Collaboration. 2015. Estimating the reproducibility of
psychological science. \emph{Science} 349:aac4716--aac4716.

\leavevmode\hypertarget{ref-Piwowar2018}{}%
Piwowar H., Priem J., Larivière V., Alperin JP., Matthias L., Norlander
B., Farley A., West J., Haustein S. 2018. The state of OA: A large-scale
analysis of the prevalence and impact of open access articles.
\emph{PeerJ} 6:e4375.

\leavevmode\hypertarget{ref-Sinclair2016}{}%
Sinclair L., Ijaz UZ., Jensen LJ., Coolen MJ., Gubry-Rangin C.,
Chroňáková A., Oulas A., Pavloudi C., Schnetzer J., Weimann A., Ijaz A.,
Eiler A., Quince C., Pafilis E. 2016. \texttt{Seqenv}: Linking sequences
to environments through text mining. \emph{PeerJ} 4:e2690.

\leavevmode\hypertarget{ref-Usai2018}{}%
Usai A., Pironti M., Mital M., Mejri CA. 2018. Knowledge discovery out
of text data: A systematic review via text mining. \emph{Journal of
Knowledge Management} 22:1471--1488.

\end{document}


